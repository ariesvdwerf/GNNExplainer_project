{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.gengraph import gen_syn1, gen_syn3, gen_syn4, preprocess_input_graph\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GCN Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Convolution Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in:int, dim_out:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_in = dim_in # input number of features\n",
    "        self.dim_out = dim_out # output size\n",
    "        # Initialize weights as suggested, using Glorot initialization\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_uniform_(torch.empty(self.dim_in,\n",
    "                                                self.dim_out,\n",
    "                                                dtype=torch.float64)))\n",
    "    \n",
    "    def forward(self, H:torch.sparse, A:torch.sparse):# -> torch.Tensor:\n",
    "        \n",
    "        # compute degree matrix:\n",
    "        # 1. Combine sum over rows and sum over columns\n",
    "        # 2. Remove identity vector from sums to deal with double self-loops\n",
    "        # 3. Take inverse square root of all elements and create diagonal matrix\n",
    "        self.D = torch.diag(1/torch.sqrt(\n",
    "            (-torch.ones(A.shape[0]) + A.sum(axis=0) + A.sum(axis=1))/2))\n",
    "        \n",
    "        # Compute Ã‚\n",
    "        self.A_hat = self.D @ A @ self.D\n",
    "        \n",
    "        return (self.A_hat @ H @ self.W) \n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.GCN1 = GraphConvolutionLayer(dim_in, dim_hidden)\n",
    "        self.GCN2 = GraphConvolutionLayer(dim_hidden, dim_out)\n",
    "\n",
    "    def encode(self, H:torch.sparse, A:torch.sparse) -> torch.Tensor:\n",
    "        h1 = self.relu(self.GCN1(H, A))\n",
    "        h1 = self.dropout(h1)\n",
    "        return self.GCN2(h1, A)\n",
    "    \n",
    "    def decode(self, node_embeds, edge_ids):\n",
    "        \"\"\" Compute edge embeddings based on node similarity\n",
    "        \n",
    "        Edge_ids stores all pairs of node ids for which we want link predictions\n",
    "        \n",
    "        ::param:: node_embeds <- torch.tensor(n_nodes, size_embeds)\n",
    "        ::param:: edge_ids <- torch.tensor(n_edges, 2)\n",
    "        \n",
    "        Return:\n",
    "            edge_embeds <- torch.tensor(n_edges, 1)\n",
    "        \"\"\"\n",
    "        edge_embeds = torch.sum(node_embeds[edge_ids.T[0]] * node_embeds[edge_ids.T[1]], dim=-1)\n",
    "        return edge_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_dict[0][\"feat\"]: float32\n",
      "G.nodes[0][\"feat\"]: float32\n"
     ]
    }
   ],
   "source": [
    "# a, b, c = gen_syn1()\n",
    "# a, b, c = gen_syn3()\n",
    "a, b, c = gen_syn4()\n",
    "\n",
    "data = preprocess_input_graph(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_split(adj, p:float=0.1):\n",
    "    \"\"\" \n",
    "    Returns train sub-graph and edge indices and labels for train and test links\n",
    "\n",
    "    ::param:: adj <- np.array : adjacency matrix\n",
    "    ::param:: p <- float : proportion of positive links to remove\n",
    "                           and of pos/neg links to train on\n",
    "    \n",
    "    Returns Tuple(torch.sparse_coo, np.array, np.array,\n",
    "                  np.array, np.array):\n",
    "        ...\n",
    "    \"\"\"\n",
    "    # Find all pos edge indices in adj\n",
    "    pos_edges = np.argwhere(adj == np.argmax(adj)).tolist()\n",
    "    np.random.shuffle(pos_edges)\n",
    "    \n",
    "    # pos edges to remove\n",
    "    len_split = int(len(pos_edges)*p)\n",
    "    pos_ids = pos_edges[:len_split]\n",
    "    pos_test = pos_edges[len_split:]\n",
    "    len_test = len(pos_test)\n",
    "    \n",
    "    # select neg edges\n",
    "    neg_edges = np.argwhere(adj == np.argmin(adj)).tolist()\n",
    "    np.random.shuffle(neg_edges)\n",
    "    neg_ids = neg_edges[:len_split] # same number as pos edges\n",
    "    neg_test = neg_edges[len_split:len_split + len_test] # same number as pos edges\n",
    "    \n",
    "    sub_g = adj.copy()\n",
    "    for id in pos_ids:\n",
    "        sub_g[id] = 0.0\n",
    "    \n",
    "    return sub_g, np.concatenate((pos_ids, neg_ids)),\\\n",
    "        np.concatenate((np.ones(len_split), np.zeros(len_split))),\\\n",
    "        np.concatenate((pos_test, neg_test)),\\\n",
    "        np.concatenate((np.ones(len_test), np.zeros(len_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_g, train_edgeids, train_labels, test_edgeids, test_labels = gen_data_split(data['adj'][0], p = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GCN on link prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7007034099539495\n",
      "Accuracy: 0.4982728958129883\n",
      "Loss: 0.6844691018922185\n",
      "Accuracy: 0.5069084763526917\n",
      "Loss: 0.6649768547591206\n",
      "Accuracy: 0.5552676916122437\n",
      "Loss: 0.6530262088429327\n",
      "Accuracy: 0.6243523359298706\n",
      "Loss: 0.637425807875001\n",
      "Accuracy: 0.6571675539016724\n",
      "Loss: 0.6259037537531857\n",
      "Accuracy: 0.6623488664627075\n",
      "Loss: 0.6106863884651867\n",
      "Accuracy: 0.6442141532897949\n",
      "Loss: 0.5976391185398781\n",
      "Accuracy: 0.6433506011962891\n",
      "Loss: 0.5855957639775985\n",
      "Accuracy: 0.6390328407287598\n",
      "Loss: 0.5699500601871834\n",
      "Accuracy: 0.6943005323410034\n",
      "Loss: 0.5583254382323078\n",
      "Accuracy: 0.7366148829460144\n",
      "Loss: 0.5437051603385071\n",
      "Accuracy: 0.7754749655723572\n",
      "Loss: 0.5295992130302981\n",
      "Accuracy: 0.8324697613716125\n",
      "Loss: 0.5170864021080103\n",
      "Accuracy: 0.8747841119766235\n",
      "Loss: 0.509096340493784\n",
      "Accuracy: 0.8851467967033386\n",
      "Loss: 0.4931271124880208\n",
      "Accuracy: 0.9058721661567688\n",
      "Loss: 0.47992762395229593\n",
      "Accuracy: 0.9188255667686462\n",
      "Loss: 0.46735299808837283\n",
      "Accuracy: 0.936096727848053\n",
      "Loss: 0.45490939529962066\n",
      "Accuracy: 0.9343696236610413\n",
      "Loss: 0.4373008568644285\n",
      "Accuracy: 0.9369602799415588\n",
      "Loss: 0.4266085059953464\n",
      "Accuracy: 0.9404144883155823\n",
      "Loss: 0.41664852706562616\n",
      "Accuracy: 0.9430052042007446\n",
      "Loss: 0.40239276426888165\n",
      "Accuracy: 0.9473229646682739\n",
      "Loss: 0.3901432401071751\n",
      "Accuracy: 0.9542314410209656\n",
      "Loss: 0.3771629778758469\n",
      "Accuracy: 0.9559585452079773\n",
      "Loss: 0.36629342152856126\n",
      "Accuracy: 0.9594127535820007\n",
      "Loss: 0.350916965939963\n",
      "Accuracy: 0.9602763652801514\n",
      "Loss: 0.3374476916931327\n",
      "Accuracy: 0.9611399173736572\n",
      "Loss: 0.3267216475740512\n",
      "Accuracy: 0.9645941257476807\n",
      "Loss: 0.3159806913495791\n",
      "Accuracy: 0.9645941257476807\n",
      "Loss: 0.3051356368781886\n",
      "Accuracy: 0.957685649394989\n",
      "Loss: 0.2938505481106194\n",
      "Accuracy: 0.962867021560669\n",
      "Loss: 0.2833710141290244\n",
      "Accuracy: 0.9706390500068665\n",
      "Loss: 0.2687503639123308\n",
      "Accuracy: 0.9663212299346924\n",
      "Loss: 0.26122404603874233\n",
      "Accuracy: 0.9663212299346924\n",
      "Loss: 0.2501982653988087\n",
      "Accuracy: 0.9706390500068665\n",
      "Loss: 0.2407147920291983\n",
      "Accuracy: 0.9671847820281982\n",
      "Loss: 0.23169735505772898\n",
      "Accuracy: 0.9689119458198547\n",
      "Loss: 0.22136896421056015\n",
      "Accuracy: 0.9740932583808899\n",
      "Loss: 0.2133670521880665\n",
      "Accuracy: 0.9723661541938782\n",
      "Loss: 0.20498845041508607\n",
      "Accuracy: 0.9740932583808899\n",
      "Loss: 0.19868003067812495\n",
      "Accuracy: 0.9766839146614075\n",
      "Loss: 0.18595466347370346\n",
      "Accuracy: 0.9758203625679016\n",
      "Loss: 0.17975453418250995\n",
      "Accuracy: 0.9801381826400757\n",
      "Loss: 0.17167797268861762\n",
      "Accuracy: 0.9758203625679016\n",
      "Loss: 0.16339818068007259\n",
      "Accuracy: 0.9792746305465698\n",
      "Loss: 0.15552220873320605\n",
      "Accuracy: 0.9792746305465698\n",
      "Loss: 0.14575151987235443\n",
      "Accuracy: 0.9827288389205933\n",
      "Loss: 0.13968804916304303\n",
      "Accuracy: 0.9879102110862732\n",
      "Loss: 0.1361972717832228\n",
      "Accuracy: 0.9801381826400757\n",
      "----- Evaluate model -----\n",
      "Test Loss: 0.6164790499659438\n",
      "Test Accuracy: 0.7095343470573425\n"
     ]
    }
   ],
   "source": [
    "# Load full graph adjacency matrix as sparse tensor\n",
    "A = torch.tensor(data['adj'][0])\n",
    "A = (A + torch.eye(A.shape[0])).to_sparse_coo()\n",
    "\n",
    "# Load sub-graph as sparse tensor\n",
    "sub_g = torch.tensor(sub_g)\n",
    "sub_g = (sub_g + torch.eye(sub_g.shape[0])).to_sparse_coo()\n",
    "\n",
    "node_features = torch.eye(data['feat'][0].shape[0], dtype=torch.float64)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Define the model\n",
    "input_size = node_features.size(1)\n",
    "hidden_size = 500\n",
    "output_size = 100  # Size of node embeddings to decode \n",
    "lr = 0.001\n",
    "model = GCN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model.encode(node_features, A=sub_g)\n",
    "    preds = model.decode(outputs, train_edgeids)\n",
    "    \n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(preds, train_labels)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    acc = torch.sum((torch.sigmoid(preds) > 0.5) == train_labels)/train_labels.shape[0]\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Eval the model on test edges\n",
    "print(f\"----- Evaluate model -----\")\n",
    "model.eval()\n",
    "\n",
    "test_outputs = model.encode(node_features, A=A)\n",
    "test_preds = model.decode(test_outputs, test_edgeids)\n",
    "test_loss = criterion(test_preds, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "test_acc = torch.sum((torch.sigmoid(test_preds) > 0.5) == test_labels)/test_labels.shape[0]\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNExplainer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model, adj_matrix, labels, size_weight:int=0.005):\n",
    "        super(GNNExplainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.labels = labels\n",
    "        self.size_weight = size_weight\n",
    "        self.node_mask = torch.empty((adj_matrix.shape[0], adj_matrix.shape[1]), dtype=torch.float64)\n",
    "        torch.nn.init.xavier_uniform_(self.node_mask)\n",
    "        # torch.nn.init.constant_(self.node_mask, 1.0)\n",
    "        self.node_mask = torch.sigmoid(self.node_mask)\n",
    "        self.node_mask = nn.Parameter(self.node_mask)\n",
    "        \n",
    "    def forward(self, edge_idx, x):\n",
    "        \"\"\" \n",
    "        Use node mask to mask out unimportant nodes/edges for link prediction(s).\n",
    "        Returns probability for edge(s) in node_idx and final adjacency matrix mask\n",
    "        \n",
    "        ::param:: edge_idx <- Tuple(int, int) : indices of nodes to predict edge between\n",
    "                        or <- List(Tuple(int, int)) : for multi edge prediction\n",
    "        ::param:: x <- Tensor(N x k) : node features of dimension (number of nodes x number of features)\n",
    "        \"\"\"\n",
    "        # Use trained node_mask to mask adjacency matrix\n",
    "        sym_mask = self.node_mask\n",
    "        sym_mask = (sym_mask + sym_mask.t())/2\n",
    "        masked_adj = self.adj_matrix * sym_mask\n",
    "        \n",
    "        # Compute GCN prediction using masked adjacency matrix\n",
    "        outputs = model.encode(x, masked_adj)\n",
    "        preds = model.decode(outputs, edge_idx)\n",
    "        return preds, masked_adj\n",
    "    \n",
    "    def loss(self, pred, explain_pred):\n",
    "        entropy_loss = -torch.sum(pred * torch.log(explain_pred))\n",
    "        mask_size_loss = self.size_weight * torch.sum(torch.sigmoid(self.node_mask))\n",
    "            \n",
    "        return entropy_loss + mask_size_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Explainer on trained GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link prediction to explain\n",
    "# edge = torch.tensor([696, 697])\n",
    "# edge = torch.tensor([1019, 1018])\n",
    "edge = torch.tensor([870, 869])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aries\\AppData\\Local\\Temp\\ipykernel_14448\\3284162787.py:67: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3618.)\n",
      "  edge_embeds = torch.sum(node_embeds[edge_ids.T[0]] * node_embeds[edge_ids.T[1]], dim=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2361.59103288144, Explainer_pred:0.7161332465785349\n",
      "Loss: 2360.697646032843, Explainer_pred:0.7153812309348347\n",
      "Loss: 2359.8040532219247, Explainer_pred:0.7146349231078027\n",
      "Loss: 2358.9102511259866, Explainer_pred:0.7138943886613072\n",
      "Loss: 2358.0162364396965, Explainer_pred:0.7131596883945188\n",
      "Loss: 2357.1220058830395, Explainer_pred:0.7124308781793095\n",
      "Loss: 2356.2275562090767, Explainer_pred:0.7117080088219777\n",
      "Loss: 2355.3328842114456, Explainer_pred:0.7109911259502694\n",
      "Loss: 2354.4379867315556, Explainer_pred:0.710280269926336\n",
      "Loss: 2353.5428606654273, Explainer_pred:0.7095754757859791\n",
      "Loss: 2352.6475029701296, Explainer_pred:0.7088767732042249\n",
      "Loss: 2351.751910669782, Explainer_pred:0.7081841864869648\n",
      "Loss: 2350.8560808610832, Explainer_pred:0.7074977345881256\n",
      "Loss: 2349.9600107183524, Explainer_pred:0.7068174311515645\n",
      "Loss: 2349.063697583432, Explainer_pred:0.7061433185397172\n",
      "Loss: 2348.1671390392557, Explainer_pred:0.7054754943136265\n",
      "Loss: 2347.270332208637, Explainer_pred:0.7048138340481989\n",
      "Loss: 2346.3732746131445, Explainer_pred:0.7041583299884184\n",
      "Loss: 2345.4759638698342, Explainer_pred:0.7035089697833811\n",
      "Loss: 2344.578397693009, Explainer_pred:0.7028657366118508\n",
      "Loss: 2343.6805738955213, Explainer_pred:0.7022286093304474\n",
      "Loss: 2342.7824903895976, Explainer_pred:0.7015975626400911\n",
      "Loss: 2341.8841451872217, Explainer_pred:0.7009725672670932\n",
      "Loss: 2340.9855364000946, Explainer_pred:0.7003535901557983\n",
      "Loss: 2340.0866622391954, Explainer_pred:0.6997405946700793\n",
      "Loss: 2339.1875210139665, Explainer_pred:0.6991335408012773\n",
      "Loss: 2338.2881111311685, Explainer_pred:0.6985323853804353\n",
      "Loss: 2337.3884310934145, Explainer_pred:0.69793708229288\n",
      "Loss: 2336.4884794974364, Explainer_pred:0.697347582693409\n",
      "Loss: 2335.5882550320985, Explainer_pred:0.6967638352205143\n",
      "Loss: 2334.687756476206, Explainer_pred:0.6961857862082559\n",
      "Loss: 2333.786982696116, Explainer_pred:0.6956133798945427\n",
      "Loss: 2332.8859326432116, Explainer_pred:0.6950465586247584\n",
      "Loss: 2331.984605351234, Explainer_pred:0.6944852630497959\n",
      "Loss: 2331.082999933531, Explainer_pred:0.693929432317721\n",
      "Loss: 2330.1811155802175, Explainer_pred:0.693379004258412\n",
      "Loss: 2329.278951555296, Explainer_pred:0.6928339155606495\n",
      "Loss: 2328.3765071937437, Explainer_pred:0.692294101941244\n",
      "Loss: 2327.4737818985886, Explainer_pred:0.6917594983059026\n",
      "Loss: 2326.5707751379923, Explainer_pred:0.6912300389016278\n",
      "Loss: 2325.6674862686764, Explainer_pred:0.6907055863330847\n",
      "Loss: 2324.7639149648567, Explainer_pred:0.6901861101518512\n",
      "Loss: 2323.8600609466907, Explainer_pred:0.6896715734181363\n",
      "Loss: 2322.955923916576, Explainer_pred:0.6891619097901266\n",
      "Loss: 2322.051503627822, Explainer_pred:0.688657052823339\n",
      "Loss: 2321.1467998820763, Explainer_pred:0.6881569360807971\n",
      "Loss: 2320.241812526828, Explainer_pred:0.6876614932343178\n",
      "Loss: 2319.336541452993, Explainer_pred:0.6871706581573063\n",
      "Loss: 2318.4309865925884, Explainer_pred:0.686684365009447\n",
      "Loss: 2317.5251476260605, Explainer_pred:0.6862024284316242\n",
      "Loss: 2316.61902279516, Explainer_pred:0.6857240646766902\n",
      "Loss: 2315.7126140392616, Explainer_pred:0.6852499908208451\n",
      "Loss: 2314.805921378212, Explainer_pred:0.6847801186593879\n",
      "Loss: 2313.8989448355974, Explainer_pred:0.6843143498966929\n",
      "Loss: 2312.9916847208115, Explainer_pred:0.6838526928088711\n",
      "Loss: 2312.084141214914, Explainer_pred:0.6833950921271604\n",
      "Loss: 2311.1763145225596, Explainer_pred:0.6829414925275539\n",
      "Loss: 2310.2682048707265, Explainer_pred:0.6824918387450528\n",
      "Loss: 2309.3598125075077, Explainer_pred:0.6820460756741685\n",
      "Loss: 2308.4511377009458, Explainer_pred:0.6816041484571163\n",
      "Loss: 2307.54218073794, Explainer_pred:0.6811660025609884\n",
      "Loss: 2306.632941923194, Explainer_pred:0.6807315838450528\n",
      "Loss: 2305.723421578226, Explainer_pred:0.6803008386191991\n",
      "Loss: 2304.8136200404297, Explainer_pred:0.6798737136944483\n",
      "Loss: 2303.903537662187, Explainer_pred:0.6794501564263509\n",
      "Loss: 2302.993174810031, Explainer_pred:0.6790301147520065\n",
      "Loss: 2302.082531863856, Explainer_pred:0.6786135372213739\n",
      "Loss: 2301.1716092161782, Explainer_pred:0.6782003730234658\n",
      "Loss: 2300.2604072714325, Explainer_pred:0.6777905720079667\n",
      "Loss: 2299.3489264453215, Explainer_pred:0.6773840847027609\n",
      "Loss: 2298.437167164194, Explainer_pred:0.6769808623278089\n",
      "Loss: 2297.525129864474, Explainer_pred:0.6765808568057657\n",
      "Loss: 2296.6128149921124, Explainer_pred:0.6761840207696985\n",
      "Loss: 2295.7002230020857, Explainer_pred:0.6757903075682259\n",
      "Loss: 2294.78735443895, Explainer_pred:0.6753997053219776\n",
      "Loss: 2293.874209924609, Explainer_pred:0.6750122306091682\n",
      "Loss: 2292.960789727966, Explainer_pred:0.6746277507378002\n",
      "Loss: 2292.047094333184, Explainer_pred:0.674246221053418\n",
      "Loss: 2291.133124231624, Explainer_pred:0.6738675977057733\n",
      "Loss: 2290.2188799214828, Explainer_pred:0.6734918376342369\n",
      "Loss: 2289.304361907444, Explainer_pred:0.6731188985531235\n",
      "Loss: 2288.389570755755, Explainer_pred:0.672748762312085\n",
      "Loss: 2287.4745074256775, Explainer_pred:0.6723815708621549\n",
      "Loss: 2286.5591719751023, Explainer_pred:0.6720170887508837\n",
      "Loss: 2285.643564927284, Explainer_pred:0.6716552752160481\n",
      "Loss: 2284.727686811044, Explainer_pred:0.6712960903450992\n",
      "Loss: 2283.8115381604975, Explainer_pred:0.6709394950461549\n",
      "Loss: 2282.8951195110944, Explainer_pred:0.6705854494494183\n",
      "Loss: 2281.9784314034823, Explainer_pred:0.6702339146545024\n",
      "Loss: 2281.061474392025, Explainer_pred:0.669884856322422\n",
      "Loss: 2280.144249029722, Explainer_pred:0.6695382383966787\n",
      "Loss: 2279.2267558737753, Explainer_pred:0.6691940255149349\n",
      "Loss: 2278.3089954854163, Explainer_pred:0.6688521829878897\n",
      "Loss: 2277.3909687586724, Explainer_pred:0.6685128165060327\n",
      "Loss: 2276.472676255878, Explainer_pred:0.668175888490394\n",
      "Loss: 2275.5541182851994, Explainer_pred:0.6678412541508804\n",
      "Loss: 2274.6352954190857, Explainer_pred:0.6675088800191159\n",
      "Loss: 2273.7162082336495, Explainer_pred:0.6671787333342475\n",
      "Loss: 2272.796857308525, Explainer_pred:0.6668507820146626\n",
      "Loss: 2271.8772452734415, Explainer_pred:0.6665258666741798\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "input_size = node_features.size(1)\n",
    "hidden_size = 100\n",
    "output_size = 1  # Output size matches the number of nodes for link prediction\n",
    "lr = 0.001\n",
    "explainer = GNNExplainer(model=model, adj_matrix=A, labels=...)\n",
    "\n",
    "# Make sure GCN is set to eval\n",
    "model.eval()\n",
    "model.GCN1.W.requires_grad = False\n",
    "model.GCN2.W.requires_grad = False\n",
    "\n",
    "# GCN prediction\n",
    "gcn_pred = torch.sigmoid(model.decode(model.encode(node_features, A), edge))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "explain_optimizer = torch.optim.Adam(explainer.parameters(), lr=lr)\n",
    "\n",
    "# Training loop (replace with your actual training loop)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    explain_optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "    # Forward pass\n",
    "    outputs, _ = explainer(edge, node_features)\n",
    "\n",
    "    # Compute the loss\n",
    "    explain_loss = explainer.loss(outputs, gcn_pred)\n",
    "    print(f\"Loss: {explain_loss}\")\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    explain_loss.backward()\n",
    "    explain_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize most important sub-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_adj(adj, threshold=0.5):\n",
    "    \"\"\" Filters graph to only include edges above threshold\n",
    "    \"\"\"\n",
    "    filt_adj = adj.to_dense().detach().clone()\n",
    "    filt_adj[adj.to_dense()<threshold] = 0\n",
    "    filt_adj = filt_adj.fill_diagonal_(0)\n",
    "    return filt_adj\n",
    "\n",
    "f = filter_adj(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1, 1, figsize=(10,10))\n",
    "\n",
    "\n",
    "# Plot subgraph\n",
    "ax.set_title(\"\")\n",
    "G = nx.from_numpy_array(np.array(f))\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "# color predicted edge\n",
    "# u, v = (696, 697)\n",
    "# u, v = (1018, 1019)\n",
    "u, v = (869, 870)\n",
    "edge_colors = ['red' if e == (u, v) else 'black' for e in G.edges ]\n",
    "\n",
    "# draw sub-graph\n",
    "nx.draw_networkx(G, ax=ax, edge_color=edge_colors)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{c}_sub_graph.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
