{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.gengraph import gen_syn1, gen_syn3, gen_syn4, preprocess_input_graph\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GCN Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Graph Convolution Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in:int, dim_out:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_in = dim_in # input number of features\n",
    "        self.dim_out = dim_out # output size\n",
    "        # Initialize weights as suggested, using Glorot initialization\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_uniform_(torch.empty(self.dim_in,\n",
    "                                                self.dim_out,\n",
    "                                                dtype=torch.float64)))\n",
    "    \n",
    "    def forward(self, H:torch.sparse, A:torch.sparse):# -> torch.Tensor:\n",
    "        \n",
    "        # compute degree matrix:\n",
    "        # 1. Combine sum over rows and sum over columns\n",
    "        # 2. Remove identity vector from sums to deal with double self-loops\n",
    "        # 3. Take inverse square root of all elements and create diagonal matrix\n",
    "        self.D = torch.diag(1/torch.sqrt(\n",
    "            (-torch.ones(A.shape[0]) + A.sum(axis=0) + A.sum(axis=1))/2))\n",
    "        \n",
    "        # Compute Ã‚\n",
    "        self.A_hat = self.D @ A @ self.D\n",
    "        \n",
    "        return (self.A_hat @ H @ self.W) \n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.GCN1 = GraphConvolutionLayer(dim_in, dim_hidden)\n",
    "        self.GCN2 = GraphConvolutionLayer(dim_hidden, dim_out)\n",
    "\n",
    "    def encode(self, H:torch.sparse, A:torch.sparse) -> torch.Tensor:\n",
    "        h1 = self.relu(self.GCN1(H, A))\n",
    "        h1 = self.dropout(h1)\n",
    "        return self.GCN2(h1, A)\n",
    "    \n",
    "    def decode(self, node_embeds, edge_ids):\n",
    "        \"\"\" Compute edge embeddings based on node similarity\n",
    "        \n",
    "        Edge_ids stores all pairs of node ids for which we want link predictions\n",
    "        \n",
    "        ::param:: node_embeds <- torch.tensor(n_nodes, size_embeds)\n",
    "        ::param:: edge_ids <- torch.tensor(n_edges, 2)\n",
    "        \n",
    "        Return:\n",
    "            edge_embeds <- torch.tensor(n_edges, 1)\n",
    "        \"\"\"\n",
    "        edge_embeds = torch.sum(node_embeds[edge_ids.T[0]] * node_embeds[edge_ids.T[1]], dim=-1)\n",
    "        return edge_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_dict[0][\"feat\"]: float32\n",
      "G.nodes[0][\"feat\"]: float32\n"
     ]
    }
   ],
   "source": [
    "a, b, c = gen_syn1(width_basis=400) # BA-base graph with house motifs\n",
    "# a, b, c = gen_syn3() # BA-base graph with grid motif\n",
    "# a, b, c = gen_syn4() # Tree-base graph with cycle motif\n",
    "\n",
    "data = preprocess_input_graph(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_split(adj, p:float=0.1):\n",
    "    \"\"\" \n",
    "    Returns train sub-graph and edge indices and labels for train and test links\n",
    "\n",
    "    ::param:: adj <- np.array : adjacency matrix\n",
    "    ::param:: p <- float : proportion of positive links to remove\n",
    "                           and of pos/neg links to train on\n",
    "    \n",
    "    Returns Tuple(torch.sparse_coo, np.array, np.array,\n",
    "                  np.array, np.array):\n",
    "        ...\n",
    "    \"\"\"\n",
    "    # Find all pos edge indices in adj\n",
    "    pos_edges = np.argwhere(adj == np.argmax(adj)).tolist()\n",
    "    np.random.shuffle(pos_edges)\n",
    "    \n",
    "    # pos edges to remove\n",
    "    len_split = int(len(pos_edges)*p)\n",
    "    pos_ids = pos_edges[:len_split]\n",
    "    pos_test = pos_edges[len_split:]\n",
    "    len_test = len(pos_test)\n",
    "    \n",
    "    # select neg edges\n",
    "    neg_edges = np.argwhere(adj == np.argmin(adj)).tolist()\n",
    "    np.random.shuffle(neg_edges)\n",
    "    neg_ids = neg_edges[:len_split] # same number as pos edges\n",
    "    neg_test = neg_edges[len_split:len_split + len_test] # same number as pos edges\n",
    "    \n",
    "    sub_g = adj.copy()\n",
    "    for id in pos_ids:\n",
    "        sub_g[id] = 0.0\n",
    "    \n",
    "    return sub_g, np.concatenate((pos_ids, neg_ids)),\\\n",
    "        np.concatenate((np.ones(len_split), np.zeros(len_split))),\\\n",
    "        np.concatenate((pos_test, neg_test)),\\\n",
    "        np.concatenate((np.ones(len_test), np.zeros(len_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_g, train_edgeids, train_labels, test_edgeids, test_labels = gen_data_split(data['adj'][0], p = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GCN on link prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7414284775989302\n",
      "Accuracy: 0.4996744692325592\n",
      "Loss: 0.6970053935885175\n",
      "Accuracy: 0.5087890625\n",
      "Loss: 0.6777322860621638\n",
      "Accuracy: 0.5462239384651184\n",
      "Loss: 0.6644049752254281\n",
      "Accuracy: 0.6158854365348816\n",
      "Loss: 0.6563650178823641\n",
      "Accuracy: 0.6285807490348816\n",
      "Loss: 0.6456386416664407\n",
      "Accuracy: 0.6617838740348816\n",
      "Loss: 0.6343771547199324\n",
      "Accuracy: 0.6598307490348816\n",
      "Loss: 0.6245903826080803\n",
      "Accuracy: 0.673828125\n",
      "Loss: 0.6104743737128465\n",
      "Accuracy: 0.6809895634651184\n",
      "Loss: 0.5979431568412664\n",
      "Accuracy: 0.69140625\n",
      "Loss: 0.5827991449259714\n",
      "Accuracy: 0.7327473759651184\n",
      "Loss: 0.57218910068958\n",
      "Accuracy: 0.7431640625\n",
      "Loss: 0.5575920822186576\n",
      "Accuracy: 0.791015625\n",
      "Loss: 0.542615892772539\n",
      "Accuracy: 0.8212890625\n",
      "Loss: 0.5276067533752035\n",
      "Accuracy: 0.8421223759651184\n",
      "Loss: 0.5151331950118647\n",
      "Accuracy: 0.8551432490348816\n",
      "Loss: 0.5014314647722502\n",
      "Accuracy: 0.8720703125\n",
      "Loss: 0.49080926391899976\n",
      "Accuracy: 0.8834635615348816\n",
      "Loss: 0.47346601568984886\n",
      "Accuracy: 0.890625\n",
      "Loss: 0.45876099021866173\n",
      "Accuracy: 0.9036458134651184\n",
      "Loss: 0.4471374682880281\n",
      "Accuracy: 0.9078776240348816\n",
      "Loss: 0.4321095929077798\n",
      "Accuracy: 0.9192708134651184\n",
      "Loss: 0.4172296512718849\n",
      "Accuracy: 0.9332682490348816\n",
      "Loss: 0.40269726548348617\n",
      "Accuracy: 0.9303385615348816\n",
      "Loss: 0.3865136856899218\n",
      "Accuracy: 0.9319661259651184\n",
      "Loss: 0.3742418396834313\n",
      "Accuracy: 0.9332682490348816\n",
      "Loss: 0.36035218601907487\n",
      "Accuracy: 0.9407551884651184\n",
      "Loss: 0.3461424548473809\n",
      "Accuracy: 0.9420573115348816\n",
      "Loss: 0.3305013199249125\n",
      "Accuracy: 0.943359375\n",
      "Loss: 0.31949856186467257\n",
      "Accuracy: 0.9371744990348816\n",
      "Loss: 0.30626796779794163\n",
      "Accuracy: 0.9430338740348816\n",
      "Loss: 0.293865694297393\n",
      "Accuracy: 0.9381510615348816\n",
      "Loss: 0.2789029043056519\n",
      "Accuracy: 0.9466145634651184\n",
      "Loss: 0.2724335897243488\n",
      "Accuracy: 0.9436848759651184\n",
      "Loss: 0.2593266319754091\n",
      "Accuracy: 0.9479166865348816\n",
      "Loss: 0.24790456178558085\n",
      "Accuracy: 0.9479166865348816\n",
      "Loss: 0.2346889659210409\n",
      "Accuracy: 0.9521484375\n",
      "Loss: 0.22653925142420742\n",
      "Accuracy: 0.9567057490348816\n",
      "Loss: 0.21519585225217827\n",
      "Accuracy: 0.9527994990348816\n",
      "Loss: 0.2056240136866109\n",
      "Accuracy: 0.9557291865348816\n",
      "Loss: 0.2011639463557675\n",
      "Accuracy: 0.955078125\n",
      "Loss: 0.19142826458532605\n",
      "Accuracy: 0.9596354365348816\n",
      "Loss: 0.182545453111531\n",
      "Accuracy: 0.9593098759651184\n",
      "Loss: 0.17677795982262898\n",
      "Accuracy: 0.9635416865348816\n",
      "Loss: 0.1658255628078399\n",
      "Accuracy: 0.9602864384651184\n",
      "Loss: 0.15735920827448865\n",
      "Accuracy: 0.9654948115348816\n",
      "Loss: 0.15308759388920568\n",
      "Accuracy: 0.9674479365348816\n",
      "Loss: 0.14170408679614968\n",
      "Accuracy: 0.9703776240348816\n",
      "Loss: 0.13855102861709523\n",
      "Accuracy: 0.9713541865348816\n",
      "Loss: 0.13498586250416286\n",
      "Accuracy: 0.9680989384651184\n",
      "----- Evaluate model -----\n",
      "Test Loss: 0.6657669000848127\n",
      "Test Accuracy: 0.7361885905265808\n"
     ]
    }
   ],
   "source": [
    "# Load full graph adjacency matrix as sparse tensor\n",
    "A = torch.tensor(data['adj'][0])\n",
    "A = (A + torch.eye(A.shape[0])).to_sparse_coo()\n",
    "\n",
    "# Load sub-graph as sparse tensor\n",
    "sub_g = torch.tensor(sub_g)\n",
    "sub_g = (sub_g + torch.eye(sub_g.shape[0])).to_sparse_coo()\n",
    "\n",
    "node_features = torch.eye(data['feat'][0].shape[0], dtype=torch.float64)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Define the model\n",
    "input_size = node_features.size(1)\n",
    "hidden_size = 500\n",
    "output_size = 100  # Size of node embeddings to decode \n",
    "lr = 0.001\n",
    "model = GCN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model.encode(node_features, A=sub_g)\n",
    "    preds = model.decode(outputs, train_edgeids)\n",
    "    \n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(preds, train_labels)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    acc = torch.sum((torch.sigmoid(preds) > 0.5) == train_labels)/train_labels.shape[0]\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Eval the model on test edges\n",
    "print(f\"----- Evaluate model -----\")\n",
    "model.eval()\n",
    "\n",
    "test_outputs = model.encode(node_features, A=A)\n",
    "test_preds = model.decode(test_outputs, test_edgeids)\n",
    "test_loss = criterion(test_preds, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "test_acc = torch.sum((torch.sigmoid(test_preds) > 0.5) == test_labels)/test_labels.shape[0]\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNExplainer(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model, adj_matrix, labels, size_weight:int=0.000005):\n",
    "        super(GNNExplainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.adj_matrix = adj_matrix\n",
    "        self.labels = labels\n",
    "        self.size_weight = size_weight\n",
    "        self.node_mask = torch.empty((adj_matrix.shape[0], adj_matrix.shape[1]), dtype=torch.float64)\n",
    "        torch.nn.init.xavier_uniform_(self.node_mask)\n",
    "        # torch.nn.init.constant_(self.node_mask, 1.0)\n",
    "        self.node_mask = torch.sigmoid(self.node_mask)\n",
    "        self.node_mask = nn.Parameter(self.node_mask)\n",
    "        \n",
    "    def forward(self, edge_idx, x):\n",
    "        \"\"\" \n",
    "        Use node mask to mask out unimportant nodes/edges for link prediction(s).\n",
    "        Returns probability for edge(s) in node_idx and final adjacency matrix mask\n",
    "        \n",
    "        ::param:: edge_idx <- Tuple(int, int) : indices of nodes to predict edge between\n",
    "                        or <- List(Tuple(int, int)) : for multi edge prediction\n",
    "        ::param:: x <- Tensor(N x k) : node features of dimension (number of nodes x number of features)\n",
    "        \"\"\"\n",
    "        # Use trained node_mask to mask adjacency matrix\n",
    "        sym_mask = self.node_mask\n",
    "        sym_mask = (sym_mask + sym_mask.t())/2\n",
    "        masked_adj = self.adj_matrix * sym_mask\n",
    "        \n",
    "        # Compute GCN prediction using masked adjacency matrix\n",
    "        outputs = model.encode(x, masked_adj)\n",
    "        preds = model.decode(outputs, edge_idx)\n",
    "        return preds, masked_adj\n",
    "    \n",
    "    def loss(self, pred, explain_pred):\n",
    "        entropy_loss = -torch.sum(pred * torch.log(explain_pred))\n",
    "        mask_size_loss = self.size_weight * torch.sum(torch.sigmoid(self.node_mask))\n",
    "            \n",
    "        return entropy_loss + mask_size_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Explainer on trained GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link prediction to explain\n",
    "edge = torch.tensor([696, 697]) # House motif\n",
    "# edge = torch.tensor([1017, 1018]) # Grid motif\n",
    "# edge = torch.tensor([870, 869]) # Cycle motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2410736936887408\n",
      "Loss: 2.23945775949437\n",
      "Loss: 2.2378493098274372\n",
      "Loss: 2.2362483566277436\n",
      "Loss: 2.2346547231242813\n",
      "Loss: 2.233068429937126\n",
      "Loss: 2.2314894947341086\n",
      "Loss: 2.22991791932833\n",
      "Loss: 2.2283537171853243\n",
      "Loss: 2.2267968988854845\n",
      "Loss: 2.2252474636501187\n",
      "Loss: 2.223705405653925\n",
      "Loss: 2.2221707140576323\n",
      "Loss: 2.2206433730673876\n",
      "Loss: 2.2191233620183253\n",
      "Loss: 2.2176106554802257\n",
      "Loss: 2.2161052233833582\n",
      "Loss: 2.2146070311626014\n",
      "Loss: 2.213116061258851\n",
      "Loss: 2.211632338227392\n",
      "Loss: 2.2101557972338703\n",
      "Loss: 2.208686321115435\n",
      "Loss: 2.20722385386383\n",
      "Loss: 2.2057680383217733\n",
      "Loss: 2.2043190264668704\n",
      "Loss: 2.2028770081816837\n",
      "Loss: 2.201441697186585\n",
      "Loss: 2.200013065241657\n",
      "Loss: 2.1985910416740273\n",
      "Loss: 2.1971755537136697\n",
      "Loss: 2.19576652671598\n",
      "Loss: 2.1943638843775117\n",
      "Loss: 2.1929675489443134\n",
      "Loss: 2.1915774414123796\n",
      "Loss: 2.190193446513727\n",
      "Loss: 2.1888153895986147\n",
      "Loss: 2.1874434545342907\n",
      "Loss: 2.1860774253436905\n",
      "Loss: 2.1847172201139893\n",
      "Loss: 2.1833627566171705\n",
      "Loss: 2.182013952457561\n",
      "Loss: 2.1806706813418075\n",
      "Loss: 2.1793324427551157\n",
      "Loss: 2.177999585712214\n",
      "Loss: 2.17667203258957\n",
      "Loss: 2.175349705703006\n",
      "Loss: 2.174032527451717\n",
      "Loss: 2.1727204204474795\n",
      "Loss: 2.1714133076303237\n",
      "Loss: 2.1701111123717953\n",
      "Loss: 2.1688137585668446\n",
      "Loss: 2.1675211707152826\n",
      "Loss: 2.1662332739936723\n",
      "Loss: 2.164949994318443\n",
      "Loss: 2.163671258400972\n",
      "Loss: 2.1623969937953027\n",
      "Loss: 2.161127128939132\n",
      "Loss: 2.1598615931886456\n",
      "Loss: 2.1586003168477443\n",
      "Loss: 2.157343231192154\n",
      "Loss: 2.1560902684888887\n",
      "Loss: 2.154841362011494\n",
      "Loss: 2.153596446051463\n",
      "Loss: 2.1523554559261946\n",
      "Loss: 2.1511183279838417\n",
      "Loss: 2.1498849996053417\n",
      "Loss: 2.148655409203944\n",
      "Loss: 2.147429496222475\n",
      "Loss: 2.1462072011286066\n",
      "Loss: 2.1449884654083378\n",
      "Loss: 2.143773231557899\n",
      "Loss: 2.1425614430742757\n",
      "Loss: 2.1413530444445104\n",
      "Loss: 2.1401479811339508\n",
      "Loss: 2.138946199573584\n",
      "Loss: 2.1377476471465875\n",
      "Loss: 2.136552272174209\n",
      "Loss: 2.135360024247696\n",
      "Loss: 2.134170857344753\n",
      "Loss: 2.132984718684403\n",
      "Loss: 2.131801534496266\n",
      "Loss: 2.130621277626513\n",
      "Loss: 2.1294439048678484\n",
      "Loss: 2.128269370959225\n",
      "Loss: 2.127097631421739\n",
      "Loss: 2.1259286425453507\n",
      "Loss: 2.1247623613753355\n",
      "Loss: 2.1235991116130353\n",
      "Loss: 2.122438667596521\n",
      "Loss: 2.121280842335445\n",
      "Loss: 2.120125591752923\n",
      "Loss: 2.1189728728772055\n",
      "Loss: 2.1178226437859218\n",
      "Loss: 2.1166748635544814\n",
      "Loss: 2.1155294922082515\n",
      "Loss: 2.114386700402076\n",
      "Loss: 2.1132459170690137\n",
      "Loss: 2.112107347947647\n",
      "Loss: 2.110971002570685\n",
      "Loss: 2.1098368468431814\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "input_size = node_features.size(1)\n",
    "hidden_size = 100\n",
    "output_size = 1  # Output size matches the number of nodes for link prediction\n",
    "lr = 0.001\n",
    "explainer = GNNExplainer(model=model, adj_matrix=A, labels=...)\n",
    "\n",
    "# Make sure GCN is set to eval\n",
    "model.eval()\n",
    "model.GCN1.W.requires_grad = False\n",
    "model.GCN2.W.requires_grad = False\n",
    "\n",
    "# GCN prediction\n",
    "gcn_pred = torch.sigmoid(model.decode(model.encode(node_features, A), edge))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "explain_optimizer = torch.optim.Adam(explainer.parameters(), lr=lr)\n",
    "\n",
    "# Training loop (replace with your actual training loop)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    explain_optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs, _ = explainer(edge, node_features)\n",
    "\n",
    "    # Compute the loss\n",
    "    explain_loss = explainer.loss(outputs, gcn_pred)\n",
    "    print(f\"Loss: {explain_loss}\")\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    explain_loss.backward()\n",
    "    explain_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize most important sub-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_adj(adj, threshold=0.5):\n",
    "    \"\"\" Filters graph to only include edges above threshold\n",
    "    \"\"\"\n",
    "    filt_adj = adj.to_dense().detach().clone()\n",
    "    filt_adj[adj.to_dense()<threshold] = 0\n",
    "    filt_adj = filt_adj.fill_diagonal_(0) # set self-loops to 0\n",
    "    return filt_adj\n",
    "\n",
    "f = filter_adj(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1, 1, figsize=(10,10))\n",
    "\n",
    "# Plot subgraph\n",
    "ax.set_title(\"\")\n",
    "G = nx.from_numpy_array(np.array(f))\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "# color predicted edge\n",
    "u, v = edge\n",
    "u, v = u.item(), v.item()\n",
    "edge_colors = ['red' if e == (u, v) else 'black' for e in G.edges ]\n",
    "\n",
    "# draw sub-graph\n",
    "nx.draw_networkx(G, ax=ax, edge_color=edge_colors)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{c}_sub_graph.png\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
